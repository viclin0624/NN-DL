# 2.1 基本概念
* 数据集：训练集和测试集
* 特征向量：<a href="https://www.codecogs.com/eqnedit.php?latex=x&space;=&space;[x_1,...,x_D]^T" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x&space;=&space;[x_1,...,x_D]^T" title="x = [x_1,...,x_D]^T" /></a>
* 样本独立同分布
* 预测标签或标签的条件概率
* 准确率

# 2.2 机器学习的三个基本要素
## 2.2.1 模型
* 机器学习的目标是找到一个模型来近似真实映射函数或真实条件概率分布
* 假设空间：参数化的函数族
1. 线性模型
2. 非线性模型
## 2.2.2 学习准则
* 样本分布必须是固定的
* 期望风险：<a href="https://www.codecogs.com/eqnedit.php?latex=R(\theta)&space;=&space;E_{\{(x,y)\sim&space;p_r(x,y)\}}&space;L(y,f(x;\theta))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R(\theta)&space;=&space;E_{\{(x,y)\sim&space;p_r(x,y)\}}&space;L(y,f(x;\theta))" title="R(\theta) = E_{\{(x,y)\sim p_r(x,y)\}} L(y,f(x;\theta))" /></a>
### 2.2.2.1 损失函数
* 0-1损失函数
* 平方损失函数
* 交叉熵损失函数
* Hinge损失函数
### 2.2.2.2 风险最小化准则
* 经验风险：
* 过拟合（面试常考）：
  * 正则化
  * 结构风险最小化准则（Structure Risk Minimization，SRM）：<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;^*&space;=argmin_\theta&space;R_{D}^{struct}(\theta)&space;=&space;argmin_\theta&space;\frac{1}{N}\sum&space;^{N}_{n=1}&space;L(y^{(n)},f(x^{(n)};\theta))&plus;\frac{1}{2}\lambda&space;\|\|\theta\|\|^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;^*&space;=argmin_\theta&space;R_{D}^{struct}(\theta)&space;=&space;argmin_\theta&space;\frac{1}{N}\sum&space;^{N}_{n=1}&space;L(y^{(n)},f(x^{(n)};\theta))&plus;\frac{1}{2}\lambda&space;\|\|\theta\|\|^2" title="\theta ^* =argmin_\theta R_{D}^{struct}(\theta) = argmin_\theta \frac{1}{N}\sum ^{N}_{n=1} L(y^{(n)},f(x^{(n)};\theta))+\frac{1}{2}\lambda \|\|\theta\|\|^2" /></a>
  * l_1范数使得参数更加具有稀疏性
## 2.2.3 优化算法
* 参数与超参数
* 梯度下降法
* 提前停止（防止过拟合）
* 随机梯度下降法
* 小批量梯度下降法
# 2.3 机器学习的简单示例——线性回归
## 2.3.1 参数学习
* 经验风险最小化
* 结构风险最小化
* 最大似然估计
* 最大后验估计
# 2.4 偏差-方差分解
* 期望错误可以分解为当前模型与最优模型的差距与由样本分布和噪声引起的损失
* 对单个样本，不同训练集得到的模型和最优模型之间的期望差距可以分解为Bias与Variance
* 最小化期望错误等价于最小化偏差与方差之和
* 降低方差：通过集成模型，多个高方差模型的平均
# 2.5 机器学习算法的类型
* 监督学习：期望风险最小化，最大似然估计
  * 回归
  * 分类
  * 结构化学习
* 无监督学习：最大似然估计，最小重构错误
* 强化学习：策略评估，策略改进
* 弱监督学习
* 半监督学习
# 2.6 数据的特征表示
* 图像特征
* 文本特征
  * 词袋模型（Bag-of-Words）
  * N元特征（N-gram）
* 表示学习
## 2.6.1 传统的特征学习
特征选择：选取原始特征集合的一个有效子集，使得基于这个特征子集训练出来的模型准确率最高  
1. 子集搜索：
  * 前向搜索
  * 反向搜索
  
