# 2.1 基本概念
* 数据集：训练集和测试集
* 特征向量：<a href="https://www.codecogs.com/eqnedit.php?latex=x&space;=&space;[x_1,...,x_D]^T" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x&space;=&space;[x_1,...,x_D]^T" title="x = [x_1,...,x_D]^T" /></a>
* 样本独立同分布
* 预测标签或标签的条件概率
* 准确率

# 2.2 机器学习的三个基本要素
## 2.2.1 模型
* 机器学习的目标是找到一个模型来近似真实映射函数或真实条件概率分布
* 假设空间：参数化的函数族
1. 线性模型
2. 非线性模型
## 2.2.2 学习准则
* 样本分布必须是固定的
* 期望风险：<a href="https://www.codecogs.com/eqnedit.php?latex=R(\theta)&space;=&space;E_{\{(x,y)\sim&space;p_r(x,y)\}}&space;L(y,f(x;\theta))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R(\theta)&space;=&space;E_{\{(x,y)\sim&space;p_r(x,y)\}}&space;L(y,f(x;\theta))" title="R(\theta) = E_{\{(x,y)\sim p_r(x,y)\}} L(y,f(x;\theta))" /></a>
### 2.2.2.1 损失函数
* 0-1损失函数
* 平方损失函数
* 交叉熵损失函数
* Hinge损失函数
### 2.2.2.2 风险最小化准则
* 经验风险：
* 过拟合（面试常考）：
  * 正则化
  * 结构风险最小化准则（Structure Risk Minimization，SRM）：<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;^*&space;=argmin_\theta&space;R_{D}^{struct}(\theta)&space;=&space;argmin_\theta&space;\frac{1}{N}\sum&space;^{N}_{n=1}&space;L(y^{(n)},f(x^{(n)};\theta))&plus;\frac{1}{2}\lambda&space;\|\|\theta\|\|^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;^*&space;=argmin_\theta&space;R_{D}^{struct}(\theta)&space;=&space;argmin_\theta&space;\frac{1}{N}\sum&space;^{N}_{n=1}&space;L(y^{(n)},f(x^{(n)};\theta))&plus;\frac{1}{2}\lambda&space;\|\|\theta\|\|^2" title="\theta ^* =argmin_\theta R_{D}^{struct}(\theta) = argmin_\theta \frac{1}{N}\sum ^{N}_{n=1} L(y^{(n)},f(x^{(n)};\theta))+\frac{1}{2}\lambda \|\|\theta\|\|^2" /></a>
  * l_1范数使得参数更加具有稀疏性
## 2.2.3 优化算法
* 参数与超参数
* 梯度下降法
* 提前停止（防止过拟合）
* 随机梯度下降法
* 小批量梯度下降法
# 2.3 机器学习的简单示例——线性回归
## 2.3.1 参数学习
* 经验风险最小化
* 结构风险最小化
* 最大似然估计
* 最大后验估计
# 2.4 偏差-方差分解
* 期望错误可以分解为当前模型与最优模型的差距与由样本分布和噪声引起的损失
* 对单个样本，不同训练集得到的模型和最优模型之间的期望差距可以分解为Bias与Variance
* 最小化期望错误等价于最小化偏差与方差之和
* 降低方差：通过集成模型，多个高方差模型的平均
# 2.5 机器学习算法的类型
* 监督学习：期望风险最小化，最大似然估计
  * 回归
  * 分类
  * 结构化学习
* 无监督学习：最大似然估计，最小重构错误
* 强化学习：策略评估，策略改进
* 弱监督学习
* 半监督学习
# 2.6 数据的特征表示
* 图像特征
* 文本特征
  * 词袋模型（Bag-of-Words）
  * N元特征（N-gram）
* 表示学习
## 2.6.1 传统的特征学习
特征选择：选取原始特征集合的一个有效子集，使得基于这个特征子集训练出来的模型准确率最高  
 1. 子集搜索：
  * 前向搜索
  * 反向搜索
  1. 过滤式方法：不依赖具体机器学习模型的特征选择方法，每次增加最有信息量的特征或删除最没有信息量的特征，可以通过信息增益来衡量（引入特征后条件分布的熵的减少程度）
  2. 包裹式方法：使用后续机器学习模型的准确率作为评价来选择一个特征子集
 2. l1正则化
特征抽取：构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示。
 * 监督：对特定的预测任务最有用的特征，例如：线性判别分析LDA
 * 无监督：目标通常是减少冗余信息和噪声，例如：主成分分析PCA，自编码器AE
||监督学习|无监督学习|
|---|---|---|
|特征选择|标签相关的子集搜索、L1正则化、决策树|标签无关的子集搜索|
|特征抽取|线性判别分析|主成分分析，独立成分分析，流形学习，自编码器|
以上方法能够达到维数约减或降维
## 2.6.2 深度学习方法
将特征的表示学习和机器学习的预测学习有机地统一到一个模型中，建立一个端到端的学习算法，就可以有效地避免它们之间准则的不一致性，这种表示学习方法称为深度学习。  
例如神经网络将最后的输出层作为预测学习，其它层作为表示学习。
# 2.7 评价指标
某一类的评价指标：
* 准确率
* 错误率
* 精确率（查准率）
* 召回率（查全率）
* F-score <a href="https://www.codecogs.com/eqnedit.php?latex=F_c&space;=&space;\frac{(1&plus;\beta&space;^2)P_cR_c}{\beta&space;^2&space;P_c&space;&plus;&space;R_c}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?F_c&space;=&space;\frac{(1&plus;\beta&space;^2)P_cR_c}{\beta&space;^2&space;P_c&space;&plus;&space;R_c}" title="F_c = \frac{(1+\beta ^2)P_cR_c}{\beta ^2 P_c + R_c}" /></a>
所有列表的评价指标：
* 宏平均：每一类指标的算数平均值
* 微平均：每一个样本的性能指标的算数平均值
其他指标：
* AUC（Area Under Curve）
* ROC（Receiver Operating Characteristic）
* PR（Precision-Recall）
* TopN准确率
交叉验证：分为K个子集，每次选取K-1个子集训练，剩下一组验证，取K次实验的K次模型的平均作为评价。
# 2.8 理论和定理
1. PAC学习理论：近似正确与可能
2. 没有免费午餐定理：如果一个算法对某种问题有效，那么在其他一些问题上比纯随机搜索算法更差
3. 奥卡姆剃刀原则：如无必要，勿增实体
4. 丑小鸭定理：一切相似性的标准都是主观的
5. 归纳偏置：先验信息
  
